{"meta":{"title":"Kind Net","subtitle":null,"description":null,"author":"Shanyi Wang","url":"http://yoursite.com"},"pages":[{"title":"about","date":"2018-07-02T00:43:04.000Z","updated":"2018-07-02T00:43:04.265Z","comments":true,"path":"about/index-1.html","permalink":"http://yoursite.com/about/index-1.html","excerpt":"","text":""},{"title":"about","date":"2018-07-02T00:40:15.000Z","updated":"2018-07-02T00:40:15.373Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"深度学习——反向传播","slug":"deeplearning-backpropagation","date":"2018-07-21T04:52:09.000Z","updated":"2018-07-21T08:04:37.182Z","comments":true,"path":"2018/07/21/deeplearning-backpropagation/","link":"","permalink":"http://yoursite.com/2018/07/21/deeplearning-backpropagation/","excerpt":"","text":"前言全连接神经网络可以看做多个感知机的堆叠。反向传播是神经网络的基础。反向传播是以误差为主导的反向传播运动，目的是为了优化全局参数的矩阵。 正向传播正向传播很好理解。多个感知机的堆叠在一起，上一层的输出作为下一层的输入。正向传播重点需要理解的是矩阵的运算。基本运算的过程如下： Z = W^{[1]} X + b^{[1]}A = g^{[1]} (Z^{[1]})Z = W^{[2]} X + b^{[2]}A = g^{[2]} (Z^{[2]})需要注意的是权重W矩阵，偏置矩阵b, 训练矩阵X的格式。矩阵W：矩阵X：矩阵b：假设l层有J个神经元，l-1层有K个神经元,M个样本。W X 后得到的是J M的矩阵。 反向传播反向传播的所要理解的重点是误差的传播。在单感知机上，我们知道输出层的误差后很容易计算权重的偏导。神经网络是多个感知机的堆叠，我们要知道每个神经元的误差，就可以计算改神经元对应参数的偏导。反向传播就是把最后一层的误差传导到前面的神经元上。 dz^{[2]} = a^{[2]} -ydW^{[2]} = dz^{[2]}a^{[1]T}db^{[2]} = dz^{[2]}dz^{[1]} = W^{[2]T}dz^{[2]} * g^{[1]'}(z^{[1]})dW^{[1]} = dz^{[1]}x^{[T]}db^{[1]} = dz^{[1]} 代码（参考 deeplearning.ai ）正向传播：1234567891011121314151617181920def backward_propagation(parameters, cache, X, Y): W1 = parameters[\"W1\"] b1 = parameters[\"b1\"] W2 = parameters[\"W2\"] b2 = parameters[\"b2\"] Z1 = np.dot(W1, X) + b1 A1 = np.tanh(Z1) Z2 = np.dot(W2, A1) + b2 A2 = sigmoid(Z2) assert(A2.shape == (1, X.shape[1])) cache = &#123;\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2&#125; return A2, cache 反向传播12345678910111213141516171819202122def backward_propagation(parameters, cache, X, Y): m = X.shape[1] W1 = parameters[\"W1\"] W2 = parameters[\"W2\"] A1 = cache[\"A1\"] A2 = cache[\"A2\"] dZ2 = A2 - Y dW2 = np.dot(dZ2, A1.T) / m db2 = np.sum(dZ2, axis=1, keepdims=True) / m dZ1 = np.multiply(np.dot(W2.T, dZ2), (1 - np.power(A1, 2))) dW1 = np.dot(dZ1, X.T) / m db1 = np.sum(dZ1, axis=1, keepdims=True) / m grads = &#123;\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2&#125; return grads 代价函数123456789def compute_cost(A2, Y, parameters): m = Y.shape[1] # number of example logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), 1-Y) cost = -np.sum(logprobs) / m cost = np.squeeze(cost) # makes sure cost is the dimension we expect. # E.g., turns [[17]] into 17 assert(isinstance(cost, float)) return cost","categories":[],"tags":[{"name":"DeepLearning, BackPropagation","slug":"DeepLearning-BackPropagation","permalink":"http://yoursite.com/tags/DeepLearning-BackPropagation/"}]},{"title":"深度学习-卷积神经网络","slug":"deeplearning-cnn","date":"2018-07-14T12:28:19.000Z","updated":"2018-07-21T07:55:08.789Z","comments":true,"path":"2018/07/14/deeplearning-cnn/","link":"","permalink":"http://yoursite.com/2018/07/14/deeplearning-cnn/","excerpt":"","text":"卷积神经网络·卷积神经网络（Convolutional Neural Network, CNN）是神经网络的一种。它的架构一般包括卷积层，池化层，全连接层组成。卷积神经网络一般用在图像处理中。 卷积卷积可以看做是特征提取，作为一个滤波器。我们可以定义多个卷积。我们使用了n个卷积，即得到n组不同的特征，n个通道(channel)。卷积的方式在指定f * f 卷积核内，对应点相乘的累加和。例如我们的卷积核为 \\begin{matrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ \\end{matrix}如果原图有一个符合X形状的特征块，那么卷积值很高，否则很低。 池化池化是对卷积层的特征做一个下采样，得到更小的特征。池化的常用的有：1) Max pooling 2) Average pooling。 我们实际上常用为MaxPolling。 图像大小填充数量方式：Valid: no PaddingSame: 卷积后和输入同样的大小。 原图像大小是n_wn_h, 过滤器是ff, Padding 是P，步长s，最后输出的图像大小是 [(n_w+2p-f)/s+1]*[(n_h+2p-f)/s+1]优缺点优点：1） 共享卷积核 2）不需要手动选取特征缺点：1) 神经网络的特征过多 2）物理含义不明确，不知道卷积提取的特征是什么","categories":[],"tags":[{"name":"DeepLearning, CNN","slug":"DeepLearning-CNN","permalink":"http://yoursite.com/tags/DeepLearning-CNN/"}]},{"title":"深度学习-感知机","slug":"deeplearning-perception","date":"2018-07-08T08:28:42.000Z","updated":"2018-07-08T08:32:49.059Z","comments":true,"path":"2018/07/08/deeplearning-perception/","link":"","permalink":"http://yoursite.com/2018/07/08/deeplearning-perception/","excerpt":"","text":"感知机感知机(Perceptron)算法是一种很好的二分类在线算法，它要求是线性可分的模型，感知机对应于在输入的空间中将实例划分成正负样本，分离它们的是分离超平面，即判别的模型。感知机被视为一种最简单形式的前馈式人工神经网络，是一种二元线性分类器。 函数定义z = w_1*x_1 + w_2+x_2 {\\cdots} w_n*x_na = sigmod(z)损失函数(Loss Function)和代价函数(Cost Function)首先明确下损失函数概念和代价函数，感知机是通过损失函数计算偏差。损失函数：Loss(error) function 对于单个样本预计值与真实值的偏差 L(a,y) = -(yloga + (1-y)log(1-a))代价函数：Cost function 对于所有样本loss的平均值 J(w,b) = \\frac{1}{m} * \\sum_{i=1}^m -(yloga + (1-y)log(1-a))逻辑回归公式推导 逻辑回归公式向量化 代码1234567def propagate(w, b, X, Y): m = X.shape[1] A = sigmoid(np.dot(w.T, X) + b) cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A))) dw = (1 / m) * np.dot(X, (A - Y).T) db = (1 / m) * np.sum(A - Y) cost = np.squeeze(cost)","categories":[],"tags":[{"name":"DeepLearning, Perception","slug":"DeepLearning-Perception","permalink":"http://yoursite.com/tags/DeepLearning-Perception/"}]}]}