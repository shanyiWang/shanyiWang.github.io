<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Kind Net">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Kind Net">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kind Net">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Kind Net</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Kind Net</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/09/deeplearning-optimize/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Shanyi Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kind Net">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/09/deeplearning-optimize/" itemprop="url">深度学习-优化算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-09T08:17:04+08:00">
                2018-08-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Mini-batch-梯度下降算法"><a href="#Mini-batch-梯度下降算法" class="headerlink" title="Mini-batch 梯度下降算法"></a>Mini-batch 梯度下降算法</h3><p>在神经网络训练的时候，数据量会非常多。因此，我们一般会对数据进行划分。令$x^1 = {x^{1}, x^{1}, … x^{n}}$, 将数据集划分成n个批次，分开进行训练。</p>
<h3 id="指数加权平均（Exponentially-Weighted-Moving-Average）"><a href="#指数加权平均（Exponentially-Weighted-Moving-Average）" class="headerlink" title="指数加权平均（Exponentially Weighted Moving Average）"></a>指数加权平均（Exponentially Weighted Moving Average）</h3><p>指数加权平均：是以指数式递减的方式的移动平均，各数值的加权影响力随时间呈指数式递减，时间越靠近当前时刻的数据加权影响力越大。计算公式如下：</p>
<script type="math/tex; mode=display">V_0 = 0</script><script type="math/tex; mode=display">V_1 = \beta * V_0 + (1 - \beta) * \theta{_1}</script><script type="math/tex; mode=display">...</script><script type="math/tex; mode=display">V_t = \beta * V_t + (1 - \beta) * \theta{_t}</script><p>例子：</p>
<p>假设$\beta=0.9$ $V_t = 0.1\theta{_1} + 0.1*0.9\theta{_2} + …$</p>
<h3 id="指数加权平均的修正"><a href="#指数加权平均的修正" class="headerlink" title="指数加权平均的修正"></a>指数加权平均的修正</h3><p>由于$V_1=0.02\theta{_1}$，$V_2=0.0196\theta{_1} + 0.02\theta{_2}$, 导致初始$V_1, V_2$的偏差过大，需要进行修正。修正的公式如下：</p>
<script type="math/tex; mode=display">V_t = \frac {\beta * V_t + (1 - \beta) * \theta{_t}}{1 - \beta ^t}</script><h3 id="动量梯度下降法"><a href="#动量梯度下降法" class="headerlink" title="动量梯度下降法"></a>动量梯度下降法</h3><p>一般梯度算法收敛比较曲折，走了不少弯路。我们希望在纵向上可以走得慢点，横向走得快一点，所以有了动量梯度下降算法。</p>
<script type="math/tex; mode=display">V_{dw} = \beta V_{dw} + (1 - \beta)dw</script><script type="math/tex; mode=display">V_{db} = \beta V_{db} + (1 - \beta)db</script><script type="math/tex; mode=display">W = W - \alpha V_{dw}, b = b - \alpha V_{db}</script><h3 id="均方根传播（Root-Mean-Square-Prop-RMSprop"><a href="#均方根传播（Root-Mean-Square-Prop-RMSprop" class="headerlink" title="均方根传播（Root Mean Square Prop  RMSprop)"></a>均方根传播（Root Mean Square Prop  RMSprop)</h3><script type="math/tex; mode=display">S_{dw} = \beta S_{dw} + (1 - \beta)dw^2</script><script type="math/tex; mode=display">S_{db} = \beta S_{db} + (1 - \beta)db^2</script><script type="math/tex; mode=display">w = w - \alpha \frac {dw}{S_{dw}}, b = b - \alpha \frac {db}{S_{db}}</script><h3 id="Adam优化算法"><a href="#Adam优化算法" class="headerlink" title="Adam优化算法"></a>Adam优化算法</h3><p>adam 其实是Momentum 和 RMSprop 两个算法的结合。</p>
<script type="math/tex; mode=display">V_dw = \beta_1V_dw + (1 - \beta_1)dw, V_db = \beta_1V_db + (1 - \beta_1)db</script><script type="math/tex; mode=display">S_dw = \beta_2S_dw + (1 - \beta_2)dw^2, S_db = \beta_2S_db + (1 - \beta_2)db^2</script><script type="math/tex; mode=display">V_{dw}^{correct} = \frac{V_{dw}}{1-\beta_1^t}，V_{db}^{correct} = \frac{V_{db}}{1-\beta_1^t}</script><script type="math/tex; mode=display">S_{dw}^{correct} = \frac{S_{dw}}{1-\beta_2^t}，S_{db}^{correct} = \frac{S_{db}}{1-\beta_2^t}</script><script type="math/tex; mode=display">W = W - \alpha \frac {V_{dw}^{correct}}{\sqrt{S_{dw}^{correct}} + \epsilon}, b = b - \alpha \frac {V_{db}^{correct}}{\sqrt{S_{db}^{correct} } + \epsilon}</script><p>一般来说 $\beta_1 = 0.9, \beta_2 = 0.99, \epsilon = 10 ^ {-8}$</p>
<h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><p>在学习的时候，希望随着迭代次数的增加，让学习率进行衰减。</p>
<script type="math/tex; mode=display">\alpha = \frac{1}{1 + decayrate * epchnum}</script><h3 id="局部最优"><a href="#局部最优" class="headerlink" title="局部最优"></a>局部最优</h3><p>一般来说，深度学习参数足够多，很难陷入局部最优。而鞍点(saddle point)较为常见。 横向看是最低点，而纵向不是最低点。导致收敛过程缓慢。 </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/21/deeplearning-backpropagation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Shanyi Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kind Net">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/21/deeplearning-backpropagation/" itemprop="url">深度学习——反向传播</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-21T12:52:09+08:00">
                2018-07-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>全连接神经网络可以看做多个感知机的堆叠。反向传播是神经网络的基础。反向传播是以误差为主导的反向传播运动，目的是为了优化全局参数的矩阵。</p>
<h3 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h3><p>正向传播很好理解。多个感知机的堆叠在一起，上一层的输出作为下一层的输入。正向传播重点需要理解的是矩阵的运算。基本运算的过程如下：</p>
<script type="math/tex; mode=display">Z = W^{[1]} X + b^{[1]}</script><script type="math/tex; mode=display">A = g^{[1]} (Z^{[1]})</script><script type="math/tex; mode=display">Z = W^{[2]} X + b^{[2]}</script><script type="math/tex; mode=display">A = g^{[2]} (Z^{[2]})</script><p>需要注意的是权重W矩阵，偏置矩阵b, 训练矩阵X的格式。<br>矩阵W：<br><img src="/2018/07/21/deeplearning-backpropagation/1.png" alt="a1"><br>矩阵X：<br><img src="/2018/07/21/deeplearning-backpropagation/2.png" alt="a1"><br>矩阵b：<br><img src="/2018/07/21/deeplearning-backpropagation/3.png" alt="a1"><br>假设l层有J个神经元，l-1层有K个神经元,M个样本。W <em> X 后得到的是J </em> M的矩阵。 </p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>反向传播的所要理解的重点是误差的传播。在单感知机上，我们知道输出层的误差后很容易计算权重的偏导。神经网络是多个感知机的堆叠，我们要知道每个神经元的误差，就可以计算改神经元对应参数的偏导。反向传播就是把最后一层的误差传导到前面的神经元上。</p>
<script type="math/tex; mode=display">dz^{[2]} = a^{[2]} -y</script><script type="math/tex; mode=display">dW^{[2]} = dz^{[2]}a^{[1]T}</script><script type="math/tex; mode=display">db^{[2]} = dz^{[2]}</script><script type="math/tex; mode=display">dz^{[1]} = W^{[2]T}dz^{[2]} * g^{[1]'}(z^{[1]})</script><script type="math/tex; mode=display">dW^{[1]} = dz^{[1]}x^{[T]}</script><script type="math/tex; mode=display">db^{[1]} = dz^{[1]}</script><p><img src="/2018/07/21/deeplearning-backpropagation/4.png" alt="a1"> </p>
<h3 id="代码（参考-deeplearning-ai-）"><a href="#代码（参考-deeplearning-ai-）" class="headerlink" title="代码（参考 deeplearning.ai ）"></a>代码（参考 deeplearning.ai ）</h3><p>正向传播：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def backward_propagation(parameters, cache, X, Y):</span><br><span class="line"></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    </span><br><span class="line">    assert(A2.shape == (1, X.shape[1]))</span><br><span class="line">    </span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">return</span> A2, cache</span><br></pre></td></tr></table></figure></p>
<p>反向传播<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def backward_propagation(parameters, cache, X, Y):</span><br><span class="line"></span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    </span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">	</span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = np.dot(dZ2, A1.T) / m</span><br><span class="line">    db2 = np.sum(dZ2, axis=1, keepdims=True) / m</span><br><span class="line">    dZ1 = np.multiply(np.dot(W2.T, dZ2), (1 - np.power(A1, 2)))</span><br><span class="line">    dW1 = np.dot(dZ1, X.T) / m</span><br><span class="line">    db1 = np.sum(dZ1, axis=1, keepdims=True) / m</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">return</span> grads</span><br></pre></td></tr></table></figure></p>
<p>代价函数<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def compute_cost(A2, Y, parameters):	</span><br><span class="line">    m = Y.shape[1] <span class="comment"># number of example</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), 1-Y)</span><br><span class="line">    cost = -np.sum(logprobs) / m</span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect. </span></span><br><span class="line">                                <span class="comment"># E.g., turns [[17]] into 17 </span></span><br><span class="line">    assert(isinstance(cost, <span class="built_in">float</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">return</span> cost</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/14/deeplearning-cnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Shanyi Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kind Net">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/14/deeplearning-cnn/" itemprop="url">深度学习-卷积神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-14T20:28:19+08:00">
                2018-07-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><p>·卷积神经网络（Convolutional Neural Network, CNN）是神经网络的一种。它的架构一般包括卷积层，池化层，全连接层组成。卷积神经网络一般用在图像处理中。</p>
<h4 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h4><p>卷积可以看做是特征提取，作为一个滤波器。我们可以定义多个卷积。我们使用了n个卷积，即得到n组不同的特征，n个通道(channel)。卷积的方式在指定f * f 卷积核内，对应点相乘的累加和。<br>例如我们的卷积核为 </p>
<script type="math/tex; mode=display">
\begin{matrix}
    1 & 0 & 1 \\
    0 & 1 & 0 \\
    1 & 0 & 1 \\
\end{matrix}</script><p>如果原图有一个符合X形状的特征块，那么卷积值很高，否则很低。</p>
<h4 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h4><p>池化是对卷积层的特征做一个下采样，得到更小的特征。池化的常用的有：1) Max pooling 2) Average pooling。 我们实际上常用为MaxPolling。</p>
<h4 id="图像大小"><a href="#图像大小" class="headerlink" title="图像大小"></a>图像大小</h4><p>填充数量方式：<br>Valid: no Padding<br>Same: 卷积后和输入同样的大小。</p>
<p>原图像大小是n_w<em>n_h, 过滤器是f</em>f, Padding 是P，步长s，最后输出的图像大小是</p>
<script type="math/tex; mode=display">[(n_w+2p-f)/s+1]*[(n_h+2p-f)/s+1]</script><h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>优点：1） 共享卷积核   2）不需要手动选取特征<br>缺点：1) 神经网络的特征过多  2）物理含义不明确，不知道卷积提取的特征是什么</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/08/deeplearning-perception/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Shanyi Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kind Net">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/08/deeplearning-perception/" itemprop="url">深度学习-感知机</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-08T16:28:42+08:00">
                2018-07-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>感知机(Perceptron)算法是一种很好的二分类在线算法，它要求是线性可分的模型，感知机对应于在输入的空间中将实例划分成正负样本，分离它们的是分离超平面，即判别的模型。感知机被视为一种最简单形式的前馈式人工神经网络，是一种二元线性分类器。</p>
<h3 id="函数定义"><a href="#函数定义" class="headerlink" title="函数定义"></a>函数定义</h3><script type="math/tex; mode=display">z = w_1*x_1 + w_2+x_2  {\cdots} w_n*x_n</script><script type="math/tex; mode=display">a = sigmod(z)</script><h3 id="损失函数-Loss-Function-和代价函数-Cost-Function"><a href="#损失函数-Loss-Function-和代价函数-Cost-Function" class="headerlink" title="损失函数(Loss Function)和代价函数(Cost Function)"></a>损失函数(Loss Function)和代价函数(Cost Function)</h3><p>首先明确下损失函数概念和代价函数，感知机是通过损失函数计算偏差。<br>损失函数：Loss(error) function  对于单个样本预计值与真实值的偏差</p>
<script type="math/tex; mode=display">L(a,y) = -(yloga + (1-y)log(1-a))</script><p>代价函数：Cost function   对于所有样本loss的平均值</p>
<script type="math/tex; mode=display">J(w,b) = \frac{1}{m} * \sum_{i=1}^m -(yloga + (1-y)log(1-a))</script><h3 id="逻辑回归公式推导"><a href="#逻辑回归公式推导" class="headerlink" title="逻辑回归公式推导"></a>逻辑回归公式推导</h3><p><img src="/2018/07/08/deeplearning-perception/1.png" alt="a1"> </p>
<h3 id="逻辑回归公式向量化"><a href="#逻辑回归公式向量化" class="headerlink" title="逻辑回归公式向量化"></a>逻辑回归公式向量化</h3><p><img src="/2018/07/08/deeplearning-perception/2.png" alt="a2"> </p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def propagate(w, b, X, Y):</span><br><span class="line">    m = X.shape[1]  </span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)                                    </span><br><span class="line">    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))                                   </span><br><span class="line">    dw = (1 / m) * np.dot(X, (A - Y).T)</span><br><span class="line">    db = (1 / m) * np.sum(A - Y)</span><br><span class="line">    cost = np.squeeze(cost)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Shanyi Wang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Shanyi Wang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
